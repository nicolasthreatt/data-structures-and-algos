Greedy Algorithms Notes

- Introduction
    + Greedy algorithms make LOCAL OPTIMAL CHOICE at each step to reach a GLOBAL OPTIMAL SOLUTION
    + They always select the BEST OPTION at each step, without revisiting previous choices
    + Mainly use to solve optimization problems

- Explanation
    + Simple and easy to implement
    + They are based on the Greedy Choice Property:
        * The optimal solution can be reached by a series of locally optimal choices
    + Not all problems satisfy this property; greedy may fail in such cases.

- Common Use Cases
    + Problems on which greedy approach works has two properties
        * Greedy Choice Property - Global optimum can be built from local optimal
        * Optimal Substructure - Solution contains optimal solutions to subproblems

- Example:
    + Coin Change Problem:
        * Find the minimum number of coins to make a given amount
    + Case 1: Greedy Solution Works
        * Coins: 1, 5, 10, 25
        * Amount: 36
        * Greedy Solution: Always take the largest coin â‰¤ remaining amount
            25 + 10 + 1 = 3 coins (Optimal)
    + Case 2: Greedy Solution Fails
        * Coins: 1, 3, 4
        * Amount: 6
        * Greedy Solution:
            + 4 + 1 + 1 = 3 coins
        * Optimal Solution:
            + 3 + 3 = 2 coins

- Time Complexity
    + O(n)
    + O(n*log(n)), if sorting required

- Space Complexity
    - O(1)

- Pros
    + Simple and easy to understand and implement
    + Often efficient with linear or near-linear time complexity
    + Can often provide good solutions quickly
    + Requires less memory compared to some other optimization techniques

- Cons
    + Does not always guarantee an optimal solution
    + May fail if the Greedy Choice Property does not hold
    + Lack of backtracking may lead to irreversible suboptimal choices
    + Not suitable for poblems with complex dependencies

- Greedy Algorithms vs Dynamic Programming
    + Both solve optimization problems
    + GREEDY - Makes local optimal choices immediately, ignoring future effects
        * Simpler and faster, but may not always be optimal
    + Dynamic Programming - Considers previous subproblems and builds up a global solution
        * Guarantees optimality but often requires more time and space

- Useful Algorithms
    + Minimum Spanning Tree (MST) - Finding smallest tree that connects all vertices in a weighted graph
    + Knapsack (Fractional) - Selecting items with maximum value while staying within a weight constraints
    + Dijkstra's Algorithm - Finding shortest path between two vertices in a graph w/ non-negative edge weights
    + Interval Scheduling - Selecting maximum number of non-overlapping intervals
    + Huffman Coding - Constructing an optimal prefix-free binary code for data compression

- Greedy Triggers
    + Pairing/Matching
        * Greedy works if elements can be paired without affecting others
        * Example(s):
            + Character pairing for palindromes
            + Matching of elements in two arrays/lists
    + Sorting + Choosing Extremes
        * Pick Samllest/Largest/Earliest/Latest after sorting (nlog(n))
        * Example(s):
            + Interval Scheduling
            + Fractional Knapsack
    + Take As Much As Possible Now
        * Consuming resources incrementally maximizes value
        * Example(s):
            + Fractional Knapsack
    + Local Optimum -> Global Optimum
        * Local decisions that never block future optimal solutions
        * Example(s):
            + Huffman Coding
    + No Bracktracking Needed - Choices that don't constrain future decisions
    + Two Pointers (Greedy Pairing)
        * Used when elements are compared from opposite ends
        * Local pairing decisions reduce the search space
        * Common in string and array problems
        * Example(s):
            + Valid Palindrome (with at most one deletion)
            + Container With Most Water
            + Two Sum (sorted array)

